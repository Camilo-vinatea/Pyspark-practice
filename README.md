# PySpark Data Processing Lab

## ğŸ“Œ DescripciÃ³n
Este proyecto demuestra cÃ³mo usar **PySpark** para construir un flujo de procesamiento de datos distribuido.  
Se trabajan dos datasets en formato CSV, aplicando transformaciones, joins, agregaciones y exportando resultados a distintos formatos (Hive, Parquet y CSV).  

El objetivo es mostrar habilidades prÃ¡cticas en **ETL con PySpark**, incluyendo:
- Lectura de datos con inferencia de esquema.  
- Limpieza y transformaciÃ³n de columnas.  
- Joins entre DataFrames.  
- Agregaciones y cÃ¡lculos por grupo.  
- Escritura de resultados en diferentes destinos.  

---

## âš™ï¸ TecnologÃ­as utilizadas
- **Python 3.x**  
- **PySpark** (Spark SQL, DataFrames API)  
- **Hive** (para almacenamiento tabular)  
- **HDFS** (para exportar datos en Parquet y CSV)  

---

## ğŸ“‚ Flujo de trabajo

â”œâ”€â”€ Dataset1.csv
â”œâ”€â”€ Dataset2.csv
â”‚
â”œâ”€â”€ Spark DataFrames
â”‚   â”œâ”€â”€ Lectura de CSV
â”‚   â””â”€â”€ Inferencia de esquema
â”‚
â”œâ”€â”€ Transformaciones
â”‚   â”œâ”€â”€ Renombrar columnas
â”‚   â”œâ”€â”€ Agregar columnas
â”‚   â””â”€â”€ Filtrar registros
â”‚
â”œâ”€â”€ Join
â”‚   â””â”€â”€ Join por customer_id
â”‚
â”œâ”€â”€ Agregaciones
â”‚   â”œâ”€â”€ sum
â”‚   â””â”€â”€ avg
â”‚
â””â”€â”€ Resultados
    â”œâ”€â”€ Hive table
    â”‚   â””â”€â”€ customer_totals
    â”œâ”€â”€ Parquet
    â”‚   â””â”€â”€ filtered_data.parquet
    â””â”€â”€ CSV
        â””â”€â”€ total_value_per_year.csv

---

## ğŸš€ EjecuciÃ³n

### 1. Instalar dependencias
```bash
pip install pyspark findspark wget
